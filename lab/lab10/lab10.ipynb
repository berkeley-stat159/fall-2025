{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7bca698-babd-4b33-9e71-94ccedb097d2",
   "metadata": {},
   "source": [
    "# Lab Session 10: Introduction to Natural Language Processing \n",
    "\n",
    "> Friday 11-14-2025, 9AM-11AM & 1PM-3PM & 3PM-5PM\n",
    ">\n",
    "> Instructors: Instructors: [Jimmy Butler](https://statistics.berkeley.edu/people/james-butler) & [Sequoia Andrade](https://statistics.berkeley.edu/people/sequoia-rose-andrade)\n",
    "\n",
    "What's planned for today:\n",
    "1. **Text Data Loading**: We will review text data loading using a dataset of fake and real news. We will perform exploratory data analysis on it.\n",
    "2. **Text Data Processing**: We will practice text processing using the SpaCy package, including lemmatization and word frequency counts.\n",
    "3. **Topic Modeling**: We will start discussing topic modeling and demonstrate how to build topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9e3b42-c6ca-4103-9832-223a0bc7c97b",
   "metadata": {},
   "source": [
    "# Text Data Loading\n",
    "\n",
    "1. First load the \"Fake\" and \"True\" data into dataframes from csvs\n",
    "2. Make two plots:\n",
    "   - One that is the distribution of subjects over the Fake dataset\n",
    "   - One that is the distribution of subjects over the True dataset\n",
    "3. Make one plot with two lines that have the count of reports (y-axis) per month per year (x-axis) with one line for True and one line for Fake\n",
    "4. Make two plots:\n",
    "   - One that is the distribution of Text length over the Fake dataset (histogram)\n",
    "   - One that is the distribution of Text length over the True dataset (histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85aea9e9-ce74-45f1-85ee-8c862d9c4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18e88c7-8e7a-4362-8f34-bde9e491ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv(\"fake-and-real-news-dataset/Fake.csv\")\n",
    "real = pd.read_csv(\"fake-and-real-news-dataset/True.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb3bd6-ffd0-444a-865b-a6b620614379",
   "metadata": {},
   "source": [
    "# Text Data Processing with SpaCy - Tokenization, Lemmatization, Word Frequency\n",
    "\n",
    "In this section we will follow steps similar to the project. Now we will start working on simply text processing using the SpaCy package and the same dataset as part 1. To install SpaCy run in your environment:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge spacy \n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "Some important definitions:\n",
    "\n",
    "- *Token*: a single word or piece of a word\n",
    "- *Lemma*: the core component of a word, e.g., \"complete\" is the lemma for \"completed\" and \"completely\"\n",
    "- *Stop Word*: a common word that does not add semantic value, such as \"a\", \"and\", \"the\", etc.\n",
    "- *Vectorization*: representing a document as a vector where each index in the vector corresponds to a token or word and each entry is the count.\n",
    "\n",
    "In this section, we will explore the most common tokens and lemmas throughout different slices of the data. We will also develop vectorization representations of the speeches. \n",
    "\n",
    " The core steps are:\n",
    "\n",
    "1. Process the Fake and True data separately using the SpaCy nlp module (first subset the speeches!)\n",
    "2. Analyze Lemmas:\n",
    "- Create a list of all lemmas that are not stop words, punctuation, or spaces.\n",
    "- Display the top 25 lemmas for each of the True and Fake datsets. Is there any difference\n",
    "\n",
    "**Resources:**\n",
    "- https://realpython.com/natural-language-processing-spacy-python/\n",
    "- https://www.statology.org/text-preprocessing-feature-engineering-spacy/ \n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html# \n",
    "- https://www.geeksforgeeks.org/nlp/how-to-store-a-tfidfvectorizer-for-future-use-in-scikit-learn/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8704435c-b413-4de1-907a-b0689c30c5b7",
   "metadata": {},
   "source": [
    "# Topic Modeling (Optional) - LDA topic Modeling with Gensim\n",
    "- Train an LDA model with 10 topics\n",
    "- Output the top 10 words for each topic. \n",
    "- Output the topic distribution for the first speech\n",
    "- Make a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a8e1c-cb1c-454a-9bee-207fdc1b651d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython - sotu",
   "language": "python",
   "name": "sotu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
